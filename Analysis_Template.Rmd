---
title: "Analysis Template"
author: "Kaydee Barker"
date: "`r Sys.Date()`"
output: 
  pdf_document: default #set as default knit option
    toc: yes
    
  word_document:
    toc: yes
	   
	html_document:
    toc: yes #table of contents
    number_sections: true #number the table of contents items
    toc_float: yes #floats at top corner as user scrolls
    code_folding: hide #hide until user clicks to show (or could do opposite, or none)
  
---



# Set up

## Global settings

Start with a clean slate and set default settings for code chunks. Some options include:

> eval = TRUE #run code in chunk (FALSE will not run)
echo = TRUE #display code in knit output (FALSE will hide)
include = TRUE #include chunk in knit output (FALSE will not include)
message = TRUE #display code messages in knit output (FALSE will hide)
warning = TRUE #display code warnings (FALSE will hide)
error = FALSE #stop rendor when error occurs (TRUE will continue and display error messages in knit output)

```{r setup, include=FALSE}

rm(list=ls()) #clear global environment/workspace
knitr::opts_chunk$set(echo = TRUE) #default display code in knit output (FALSE will hide)

getwd #see what your current working directory is

#If you need to change your working directory, adjust the following line
#setwd("C:/...folder") #set working directory

```

## Load libraries (and install if needed)

Load libraries that are commonly used for data analysis and visualization in our field.

```{r libraries, warning=FALSE, include=FALSE}

#Create groups of packages/libraries 

##1. For data reading and manipulation - could just do tidyverse and a few others, or make group of specific libraries for faster loading
datalib <- c("dplyr", "tidyr", "readr", "read_excel", "stringr", "tibble", "reshape2", "janitor") 

##2. For data visualization (note ggplot2 is in tidyverse)
vislib <- c("ggplot2", "ggthemes", "ggfortify", "ggrepel", "plotrix", "RColorBrewer", "Cairo")

##3. For calculation and stats functions, including ecology-specific libraries
statlib <- c("car", "MASS", "rcompanion", "zoo", "lme4", "MuMIn", "emmeans", "performance", "see", "corrplot", "BAT", "vegan")

#Install packages as needed - commented out, but remove # and adjust as needed
##as a group:
#lapply(c(datalib, vislib, statlib), install.packages, character.only = TRUE)

##or individually:
#install.packages("janitor") #install janitor to help clean up data

#Load libraries
lapply(c(datalib, vislib, statlib), library, character.only = TRUE)

```

## Define custom design settings for plots

To streamline a cohesive look for your plots, you can define custom color palettes and plot settings. Here I've provided examples of how you can define color palettes, and create a custom ggplot theme. You can adjust these to suit your project's needs.

```{r design, echo=FALSE}

#Color palettes

#color palette example using hex codes
colors <- c("#3DD978", "#995551", "#D9443D", "#518464")

#color palette example with named colors
colors2 <- c("khaki", "darkolivegreen", "darkred", "bisque")

#color palette with gradient with 20 steps
colors_ramp <- colorRampPalette(c("darkgreen", "azure","goldenrod"))(20)

#Custom ggplot configurations 

#Create a custom theme for ggplot with font, legend, and background settings. Transparent allows transparent png or pdf export.
my_theme <- theme(text = element_text(family = "Arial", size = 12, color = "grey10"), #set universal font family, size, and color
              plot.title = element_text(size = 20, color = "darkblue", face="bold"), #set title size, color, bold
              plot.title.position = "plot", #set title position
              plot.subtitle = element_text(size = 16, color = "blue", face="italics"), #set subtitle size, color, italics
              legend.title = element_blank(), #remove legend title
              legend.position="bottom", legend.box = "horizontal", #show legend at the bottom of your chart with variables listed side by side rather than stacked
              legend.background = element_rect(fill='bisque', linetype="solid", size=0.5, color ="darkblue"), #set appearance of legend box with fill color, and outline size, type, and color
              panel.background = element_rect(fill='transparent'), plot.background = element_rect(fill='transparent', color=NA), #make backgrounds transparent
              panel.grid.major = element_blank(), panel.grid.minor = element_blank()) #no gridlines

```

Additionally, you can pre-define the order in which data are plotted and labels for your plots to make them more readable and consistent. This can be especially helpful if you have long, vague, or confusing variable names that you'd like to change for your plots. 

```{r labels, echo=FALSE}

#Create an order of observations within a variable to override the default order for plots
order_variable <- c("obs1", "obs4", "obs2", "obs5", "obs3")

#Labels for plots
#example change variable names to have prettier and more consistent names in your plots
lbls_variable <- c("super_long_ugly_name"="Pretty Name 1", "vague_name"="Pretty Name 2", "confusing_name"="Pretty Name 3")

```


# Prepare data

## Download data from a web source

If you need public data for your project, you can create an if/else loop to download the data if it isn't already in your working directory.

```{r download data, echo=FALSE}

#Download data

#if/else loop to download a file if the file isn't already in your working directory
if(file.exists('/file.csv')){ #if a file exists in working directory
		print('downloaded') #print result
	} else{ #if it doesn't exist
		download.file("https://...", destfile= "filename.zip") #download a zipped file from a link and name it
		unzip("filename.zip", exdir= ".") #unzip the downloaded file
		file.remove("filename.zip") #remove the zipped file after it's unzipped
	} #end loop

```

## Load in data

Load in your data here by adjusting the code in this chunk. I've included a couple possible file types.

```{r load data, echo=FALSE}

#Load in your data

#Read in csvs from your computer
dataframe1 <- read_csv("dataframe.csv") #load csv into R from current working directory
dataframe2 <- read.csv("C:/Users/...pathway/dataframe.csv") #load csv into R from any path on your computer

#Read in excel file with xlsx extension from your computer - uses read_excel library 
dataframe <- read_xlsx(
  "dataframe.xlsx", #excel file from current working directory
	sheet = "SheetName", #define tab/sheet to read  
	col_names = TRUE, #use header row for column names  
	col_types = NULL, #guess data types  
	na = "", #treat blank cells as NA  
	trim_ws = TRUE, #trim any whitespace/unused columns and rows  
	skip = 6, #skip 6 rows to get to pivot table data  
	n_max = Inf, #set maximum number of rows to include - infinity
	guess_max = min(1000, Inf), #how many rows to use to guess data types  
	progress = readxl_progress(), #display progress in reading in data  
	.name_repair = "unique" #makes sure all column names not empty or duplicated
)

```

## Cleaning and manipulating data

Your data may need a bit of manipulation to get started. For example, tidying column names, changing data types, calculating additional columns, pivoting, and combining dataframes are common needs at the beginning of analysis. Adjust the code in this section to suit the needs of your project.

### Look at and clean data

It can be helpful to look at and adjust some of the characteristics of your dataframe before you continue to analysis. I've included a few options below you can adjust or delete as needed to clean and prepare your data for analysis.

Note the Tidyverse mutate(across()) function or similar mutate_at() can be adjusted to any specific column numbers or names, data type with the added (where()) argument, or "everything" to apply to the entire dataframe. You can see more selection options in the [tidyselect documentation](https://tidyselect.r-lib.org/reference/language.html).

```{r clean data, echo=FALSE}

#Cursory look at your data to see if it needs cleaning up
head(dataframe) #view the first 6 rows
str(dataframe) #view the structure of the dataframe (columns, data types, etc.)

#Use base, dplyr (tidyverse) and janitor libraries to clean data
dataframe <- dataframe %>%
  clean_names() %>% #clean column names to lowercase, with underscores
  row_to_names(row_number = 1) %>% #set the first row as the header column
  column_to_rownames(var="variable1") %>% # sets variable column as row names
  mutate(across(where(is.character), as.factor)) %>% #define variables that should be treated as a factor (usually those containing a characters, such as "site" or "treatment")
  mutate(across(c(5:10), as.numeric)) #define numeric variables

#Look at full dataframe to see that you're satisfied with the clean-up
dataframe #view full dataframe

```

### Deal with missing values

If you have missing values (called NA values) in your data, there are a few ways you could choose to handle them to avoid errors further down the road. Note you'll still need to set an action to deal with NA values in some analyses, but making sure that all of your missing values are labelled correctly, filled in, or removed now can ensure all goes smoothly later.

#### Set missing values to NA

Functions in R have arguments to deal with NA values. For example, the argument na.rm = TRUE will remove NA values before computation in base R functions. In many other functions, na.action determines what to do with NA values. It generally defaults to na.action = na.fail, throwing an error, but na.action = na.omit will perform a function with only complete cases, i.e. omit observations with NA values. 

In order to use NA arguments though, you'd need to have missing values coded as NA. From looking at your data above, you might have noticed how missing values were handled - were they set as 0, left blank, or was another value typed in, such as "NULL"? If you had missing values input as something other than NA but should be NA, you can replace those values with NA. Below are examples from base R and tidyverse dplyr library.

```{r set NA 1, echo=FALSE}

#Replace across a dataframe with base R
dataframe[dataframe == 0] <- NA #replace 0 with NA across a dataframe
dataframe[dataframe == ""] <- NA #replace blanks with NA across a dataframe
dataframe[dataframe == "NULL"] <- NA #replace any value defined here with NA across a dataframe
is.na(dataframe) <- sapply(dataframe, is.infinite) #turn infinite values into NA values

#Replace values across a dataframe using dplyr (tidyverse)
dataframe <- dataframe %>%
	mutate(across(where(is.numeric), ~na_if(.,0))) %>% #replace 0s with NA across all numeric variables
	mutate(across(where(is.factor), ~na_if(.,""))) %>% #replace blanks with NA across all factor variables
  mutate(across(everything(), ~na_if(.,"NULL"))) #replace any value defined here with NA across all variables

#Replace across a specific column with base R
dataframe$column[dataframe$column == 0] <- NA #replace 0 with NA in a specific column

#Replace across specific columns with dplyr (tidyverse)
dataframe <- dataframe %>%
  mutate_at(vars(variable1, variable2), ~na_if(.,0)) #replace 0s with NA

```

#### Set a value to NA values

There are also instances in which you might want to do the opposite from above and turn NA values into zeros or other values. For example, you may want to use zeros instead of NA for missing values for plant cover or observations of an animal, or you may want to analyze how many missing values are in a variable. Here again you can change these using either base R or tidyverse dplyr library.

```{r set NA 2, echo=FALSE}

#Replace values across a dataframe using base R
dataframe[is.na(dataframe)] <- 0 #replace NA values with 0 across a dataframe
dataframe[is.na(dataframe)] <- "NULL" #replace NA values with NULL across a dataframe

#Replace values across a dataframe using dplyr (tidyverse)
dataframe <- dataframe %>%
	replace(is.na(.), 0) #replace NA values with 0 - can replace with any value or string

#Replace across a specific column with base R
dataframe$column[is.na(dataframe$column)] <- 0 #replace NA values with 0 in a column
dataframe$column[is.na(dataframe$column)] <- "NULL" #replace NA values with NULL in a column

#Replace across specific columns with dplyr (tidyverse)
dataframe <- dataframe %>%
  mutate_at(vars(variable1, variable2), ~replace_na(., 0)) #replace NA values with 0

```

#### Interpolate missing values

A third option that works well for some kinds of data is to interpolate the missing values so you don’t have any NA values. This option might work best for missing values in a time series when you have an earlier and later measured value. Of course this option should be approached with caution to avoid having misleading results.

Values can be interpolated for specific variables using the zoo package. Linear interpolation uses a straight line between points to estimate missing data. Polynomial interpolation is generally more accurate for most data, as it defines a complex shape from the existing data before estimating missing values. 

```{r interpolate, echo=FALSE}

#Interpolate missing values with zoo package and dplyr (tidyverse)
dataframe_int <- dataframe %>%
	mutate(variable1 = na.approx(variable1), #interpolate via linear interpolation
	variable2 = na.spline(variable2)) #interpolate via polynomial interpolation 

```

## View variances and distribution of data

Now that the data is cleaned and missing values are dealt with, it can be useful to take a look at the variances and distribution of the data. This can help you assess potential data standardization and transformation needs to meet assumptions for some analyses. Adjust column numbers below.

```{r data dist, fig.align = "center", out.width='80%', fig.cap="Histograms of raw data distributions."}

#Calculate variances for variables - the diagonal values in the matrix are variances
var(dataframe[, 5:10]) #calculate variance for columns 5-10

#ggplot histograms of data
ggplot(melt(dataframe), aes(x=value)) + geom_histogram() + facet_wrap(~variable)

```

## Data transformations

### Standardization

A common need for analyses such as ordination methods (principal components analysis (PCA), nonmetric multidimensional scaling (NMDS), redundancy analysis (RDA), etc.) is for data to have the same amount of variance across variables. For this, you need to standardize your data to allow for a more accurate comparison and analysis of the relative influence of variables that are measured at different scales and in different units.

Standardization generally involves dividing values by a calculated number to reduce variance. Z-score standardization requires dividing by the Z-score, which is the number of standard deviations each data point is from the mean. 

```{r data std, fig.align = "center", out.width='80%', fig.cap="Histograms of standardized data distributions."}

#Z-score standardize numeric data
df_std <- dataframe %>%
  mutate_if(is.numeric, ~scale(., center = TRUE, scale = TRUE))

#ggplot histograms of data
ggplot(melt(df_std, value.name = "value"),aes(x=value)) + geom_histogram() + facet_wrap(~variable)

```

### Log and arcsin transformations

If your data is not normally distributed, you may need to transform it to meet the assumptions of some statistical tests. 

Let's start with a couple widely-used transformations. If your data is right (positively) skewed, you can log transform it to make it more normally distributed. If your data is a percentage value, you can arcsin transform it to stabilize the variance.

```{r log arcsin, fig.align = "center", out.width='80%', fig.cap="Histograms of data distributions after log and arcsin transformations."}

#log and arcsin transform, and scale
df_log_asin <- dataframe %>%
  mutate(across(c(3:8), log)) %>% #log transform columns 3-8
  mutate(across(c(9:10), ~.x/100)) %>% #for columns 9 and 10 input as 0 to 100, make into between 0 and 1
  mutate(across(c(9:10), ~asin(sqrt(.x)))) #arcsin transform percentage data

#ggplot histograms of data
ggplot(melt(df_log_asin, value.name = "value"),aes(x=value)) + geom_histogram(bins=40) + facet_wrap(~variable)

```

### Hellinger transformation

Another transformation that can be useful for ecological data is the Hellinger transformation. This transformation is particularly useful for compositional data, such as species abundance data, and is often used in ordination methods.

```{r hellinger, fig.align = "center", out.width='80%', fig.cap="Histograms of data distributions after Hellinger transformation."}

#Hellinger (square root) transform with vegan library
df_hellinger <- dataframe %>%
  dplyr::select((where(is.numeric))) %>% #select only numeric columns
  decostand(., #call decostand transformation function
				 method = "hellinger", #transformation method
				 zap = FALSE, #make near zero values into zeros to avoid exaggeration
				 na.rm = TRUE) %>% #remove NA values
  bind_cols(dataframe %>% dplyr::select(!where(is.numeric))) #bind non-numeric columns back to dataframe

#ggplot histograms of data
ggplot(melt(df_hellinger, value.name = "value"),aes(x=value)) + geom_histogram(bins=40) + facet_wrap(~variable)

```

### Box-Cox transformation

While there are lots more transformation options out there, one more I'd like to highlight is the Box-Cox transformation, used to stabilize variance and make data more closely conform to a normal distribution. It calculates a lambda value for each variable to transform the data towards normality from either left or right skewness. Since most transformations are for right-skewed data, the ability of Box-Cox to adjust data in either direction is a valuable feature.

```{r boxcox, fig.align = "center", out.width='80%', fig.cap="Histograms of data distributions after Box-Cox transformation."}

#function to handle Box-Cox transformation with NA and zero values
box_cox_transform <- function(x) {
  # Remove NA values temporarily
  non_na <- !is.na(x)
  x_clean <- x[non_na]
  
  #add small constant to handle zeros (slightly larger than 0)
  min_nonzero <- min(x_clean[x_clean > 0], na.rm = TRUE)
  offset <- min_nonzero/2
  x_clean <- x_clean + offset
  
  #find optimal lambda
  bc <- boxcox(x_clean ~ 1, plotit = FALSE)
  lambda <- bc$x[which.max(bc$y)]
  
  #transform the data
  if (abs(lambda) < 1e-4) { # If lambda is close to 0
    transformed <- log(x_clean)
  } else {
    transformed <- (x_clean^lambda - 1)/lambda
  }
  
  #put NA values back
  result <- x
  result[non_na] <- transformed
  
  return(list(transformed = result, lambda = lambda, offset = offset))
}

#apply Box-Cox transformation to all numeric columns
box_cox_results <- list()
df_boxcox <- dataframe

for(col in names(dataframe)) {
  if(is.numeric(dataframe[[col]])) { 
    if(all(dataframe[[col]] >= 0, na.rm = TRUE)) { #if column has only non-negative values
      result <- box_cox_transform(dataframe[[col]])
      df_boxcox[[col]] <- result$transformed
      box_cox_results[[col]] <- list(
        lambda = result$lambda,
        offset = result$offset
      )
    }
  }
}

df_boxcox #view transformed data

#ggplot histograms of data
ggplot(melt(df_boxcox, value.name = "value"),aes(x=value)) + geom_histogram(bins=40) + facet_wrap(~variable)

```

## Calculate means, standard deviations, and standard errors

To get a good picture of your original data and how the functions you applied impacted them, it can be useful to evaluate the means, standard deviations, and standard errors of your data. Additionally, the calculated values can be used when plotting error bars.

```{r means, warning = FALSE} 

#Summary stats raw data
df_summary <- dataframe %>%
  group_by(Site) %>%
  summarise_all(funs(mean(., na.rm=TRUE), sd(., na.rm=TRUE), std.error(., na.rm=TRUE)))
df_summary #view result

#Summary stats standardized data
df_summary_std <- df_std %>%
  group_by(Site) %>%
  summarise_all(funs(mean(., na.rm=TRUE), sd(., na.rm=TRUE), std.error(., na.rm=TRUE)))
df_summary_std #view result

#Summary stats Box-Cox transformed data
df_summary_transformed <- df_boxcox %>% #select dataframe with desired transformation
  group_by(Site) %>%
  summarise_all(funs(mean(., na.rm=TRUE), sd(., na.rm=TRUE), std.error(., na.rm=TRUE)))
df_summary_transformed #view result
```

# Plot bar charts and box plots of variables of interest

Now that we have a feel for the data, it can be helpful to start by looking at basic plots to show the means and variance of key response variables across a factor of interest, such as site or treatment. Below are examples of two of the most useful plot types for this purpose: bar charts and box plots.

## Bar chart with error bars

```{r bar chart error bars, fig.cap="Mean and standard error of response per site in (unit)."}

#Change the order of observations within the x variable with pre-defined order (from Chunk 4)
df_summary$x_variable <- factor(df_summary$x_variable, levels = order_variable) #change ordering manually

#Create a bar chart with error bars
ggplot(df_summary, aes(x=x_variable, y=response_mean, fill=variable1)) + #define x and y variables, fill by variable
  geom_bar(stat = "identity", position = "dodge") + #create bar chart
  geom_errorbar(aes(ymin=response_mean-response_std.error, ymax=response_mean+response_std.error), 
                color = "grey10", width = 0.7, position = "dodge") + #add error bars, define aesthetics
  my_theme + #custom theme from Chunk 3
  scale_fill_manual(values=colors) + #use color palette from Chunk 3
  scale_x_discrete(labels=lbls_variable) + #use custom labels from Chunk 4
  ggtitle("Title of Plot") + #add plot title
  xlab("X Variable") + ylab("Response (unit)") #add axis titles

#Save figure - name, type, folder, dimensions, resolution, background specs
ggsave('response_bar.svg', path = "Figures", width = 6, height = 4, units = "in", dpi = 300, bg=NULL)

```

## Stacked bar chart

```{r stacked bar, fig.cap="Proportion of variable per variable."}

#Create a long dataframe with a variable to group by and variables that add up to 100%
percent_variable_df <- dataframe %>%
  dplyr::select(x_variable, percent_variable1, percent_variable2, percent_variable3) %>% #
  group_by(x_variable) %>% #group by variable of interest, such as site or treatment
  replace(is.na(.), 0) %>% #replace any NA values with 0
  melt(, variable.name="variable", value.name="value", id = "variable1") 

percent_variable$x_variable <- factor(percent_variable$x_variable, levels = order_variable) #change ordering manually

#Stacked bar graph
ggplot(percent_variable_df, aes(x=x_variable, y=value, fill=variable)) + #define x and y, fill by variable
  geom_bar(stat = "summary", add = "mean", width = 0.7, position="stack", colour="black") + #create stacked bar chart
  my_theme + #custom theme from Chunk 3
  scale_fill_manual(labels=c("1","2","3"), values=colors) + #use color palette from Chunk 3 and define labels
  scale_x_discrete(labels=lbls_variable) +  #use custom labels from Chunk 4
  ggtitle("Title of Plot") + #add plot title
  ylab("Proportion of Variable") + xlab("X Variable") #add axis titles

#Save figure - name, type, folder, dimensions, resolution, background specs
ggsave('percent_variable.svg', path = "Figures", width = 6, height = 4, units = "in", dpi = 300, bg=NULL)

```

## Box plot

```{r box plot, warning = FALSE, fig.cap="Shannon Index values per site."}

#Box plot of variable value, grouped by an x variable such as site or treatment
ggplot(dataframe, aes(x=x_variable, y=variable, fill=factor(x_variable))) + #define x and y, fill by variable
  geom_boxplot(aes(group=x_variable, value=variable)) + #create box plot
  my_theme + #custom theme from Chunk 3
  scale_fill_manual(values=colors) + #use color palette from Chunk 3 and define labels
  scale_x_discrete(labels=lbls_variable) +  #use custom labels from Chunk 4
  ggtitle("Title of Plot") + #add plot title
  ylab("Variable") + xlab("X Variable") #add axis titles

#Save figure - name, type, folder, dimensions, resolution, background specs
ggsave('boxplot.svg', path = "Figures", width = 6, height = 4, units = "in", dpi = 300, bg=NULL)

```

# Correlations

